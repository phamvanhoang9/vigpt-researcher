{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "hypos = []\n",
    "for root, dirs, files in os.walk(\"hypos\"):\n",
    "    for file in files:\n",
    "        if file.startswith(\"hypo\") and file.endswith(\".txt\"):\n",
    "            with open(os.path.join(root, file)) as f:\n",
    "                hypos.append([line.strip() for line in f])\n",
    "                \n",
    "ground_truths = []\n",
    "for root, dirs, files in os.walk(\"ground_truths\"):\n",
    "    for file in files:\n",
    "        if file.startswith(\"gt\") and file.endswith(\".txt\"):\n",
    "            with open(os.path.join(root, file)) as f:\n",
    "                ground_truths.append([line.strip() for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hypos), len(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def read_vi_text(text):\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "read_vi = []\n",
    "for i, gt in enumerate(ground_truths):\n",
    "    ground_truths[i] = [read_vi_text(line) for line in gt]\n",
    "    read_vi.append(ground_truths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "for ref in read_vi[0]:\n",
    "    print(textwrap.fill(ref, width=120))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize Vietnamese text by converting it to its ASCII representation and removing accents.\n",
    "    \n",
    "    Args:\n",
    "    text (str): Input Vietnamese text to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    str: Normalized text with accents removed.\n",
    "    \"\"\"\n",
    "    normalized_text = unidecode(text)\n",
    "    normalized_text = re.sub(r'\\s+', ' ', normalized_text).strip().lower()\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_hypos = [[' '.join(normalize_text(text) for text in hypo)] for hypo in hypos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normalized_hypos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ground_truths = [[' '.join(normalize_text(text) for text in ground_truth)] for ground_truth in ground_truths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normalized_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wraptext(documents, width=120):\n",
    "    for idx, doc in enumerate(documents, start=1):\n",
    "        wrapped_lines = textwrap.wrap(doc, width=width)\n",
    "        for line in wrapped_lines:\n",
    "            print(line)\n",
    "        print(\"-\" * width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(normalized_ground_truths)):\n",
    "    print(f\"Document {i + 1}:\")\n",
    "    wraptext(normalized_ground_truths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(normalized_hypos)):\n",
    "    print(f\"Document {i + 1}:\")\n",
    "    wraptext(normalized_hypos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERTScore for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BERTScore is used to measure textual similarity between candidate texts and reference texts. It considers not only exact word matches but also the overall meaning, fluency, and order of the ouput.\n",
    "\n",
    "2. BERTScore: Precision, Recall, F1\n",
    "    * Precision measures how well the candidate texts avoid introducing irrelevant content.\n",
    "    * Recall measures how well the candidate texts avoid omitting relevant content.\n",
    "    * F1 = 2 x (P x R)/(P + R) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide the loading messages\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_score(hypos, refs, lang=\"vi\"):\n",
    "    bert_scores = []\n",
    "    for hypo in hypos:\n",
    "        for ref in refs:\n",
    "            scores = score(hypo, ref, lang=lang, verbose=False)\n",
    "            bert_scores.append(scores)\n",
    "    return bert_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scores = bert_score(normalized_hypos, normalized_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bert_scores = np.array(bert_scores)\n",
    "\n",
    "print(f\"BERTScore:\")\n",
    "print(f\"Precision: {bert_scores[:, 0].mean():.2f}\")\n",
    "print(f\"Recall: {bert_scores[:, 1].mean():.2f}\")\n",
    "print(f\"F1: {bert_scores[:, 2].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ROUGE score for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def rouge_score(hypos, refs):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, hypo) for ref, hypo in zip(refs, hypos)]\n",
    "    return scores\n",
    "\n",
    "rouge_scores = []\n",
    "for i, (hypo, ref) in enumerate(zip(normalized_hypos, normalized_ground_truths), start=1):\n",
    "    rouge_scores.append(rouge_score(hypo, ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1_precisions = []\n",
    "rouge_1_recalls = []\n",
    "rouge_1_fmeasures = []\n",
    "rouge_2_precisions = []\n",
    "rouge_2_recalls = []\n",
    "rouge_2_fmeasures = []\n",
    "rouge_L_precisions = []\n",
    "rouge_L_recalls = []\n",
    "rouge_L_fmeasures = []\n",
    "\n",
    "for rouge_score in rouge_scores:\n",
    "    for scores in rouge_score:\n",
    "        rouge_1 = scores['rouge1']\n",
    "        rouge_2 = scores['rouge2']\n",
    "        rouge_L = scores['rougeL']\n",
    "        \n",
    "        rouge_1_precisions.append(rouge_1.precision)\n",
    "        rouge_1_recalls.append(rouge_1.recall)\n",
    "        rouge_1_fmeasures.append(rouge_1.fmeasure)\n",
    "        rouge_2_precisions.append(rouge_2.precision)\n",
    "        rouge_2_recalls.append(rouge_2.recall)\n",
    "        rouge_2_fmeasures.append(rouge_2.fmeasure)\n",
    "        rouge_L_precisions.append(rouge_L.precision)\n",
    "        rouge_L_recalls.append(rouge_L.recall)\n",
    "        rouge_L_fmeasures.append(rouge_L.fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1_precisions = np.array(rouge_1_precisions)\n",
    "print(f\"ROUGE-1 Precision: {rouge_1_precisions.mean():.2f}\")\n",
    "rouge_1_recalls = np.array(rouge_1_recalls)\n",
    "print(f\"ROUGE-1 Recall: {rouge_1_recalls.mean():.2f}\")\n",
    "rouge_1_fmeasures = np.array(rouge_1_fmeasures)\n",
    "print(f\"ROUGE-1 F1: {rouge_1_fmeasures.mean():.2f}\")\n",
    "rouge_2_precisions = np.array(rouge_2_precisions)\n",
    "print(f\"ROUGE-2 Precision: {rouge_2_precisions.mean():.2f}\")\n",
    "rouge_2_recalls = np.array(rouge_2_recalls)\n",
    "print(f\"ROUGE-2 Recall: {rouge_2_recalls.mean():.2f}\")\n",
    "rouge_2_fmeasures = np.array(rouge_2_fmeasures)\n",
    "print(f\"ROUGE-2 F1: {rouge_2_fmeasures.mean():.2f}\")\n",
    "rouge_L_precisions = np.array(rouge_L_precisions)\n",
    "print(f\"ROUGE-L Precision: {rouge_L_precisions.mean():.2f}\")\n",
    "rouge_L_recalls = np.array(rouge_L_recalls)\n",
    "print(f\"ROUGE-L Recall: {rouge_L_recalls.mean():.2f}\")\n",
    "rouge_L_fmeasures = np.array(rouge_L_fmeasures)\n",
    "print(f\"ROUGE-L F1: {rouge_L_fmeasures.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Perplexity for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate Perplexity score\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "\n",
    "def perplexity_score(hypos, tokenizer, model):\n",
    "    perplexity_scores = []\n",
    "    for hypo in hypos:\n",
    "        input_ids = tokenizer.encode(hypo[0], return_tensors=\"pt\")\n",
    "        # Truncate the sequence if it's longer than the model's maximum input length\n",
    "        if input_ids.size(1) > 512:\n",
    "            input_ids = input_ids[:, :512]\n",
    "        with torch.no_grad():\n",
    "            loss = model(input_ids, labels=input_ids)[0]\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        perplexity_scores.append(perplexity)\n",
    "    return perplexity_scores\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Calculate the Perplexity score\n",
    "perplexity = perplexity_score(normalized_hypos, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = np.array(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Perplexity: {perplexity.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS\n",
    "\n",
    "| METRIC   | Precision | Recall | F1 |\n",
    "|---------|----------|-------|-------|\n",
    "| BERTScore|0.81 |0.83 |0.82 |\n",
    "| ROUGE-1| 0.82|0.50 |0.61 |\n",
    "| ROUGE-2| 0.46|0.28 | 0.35|\n",
    "| ROUGE-L| 0.40|0.25 |0.30 |\n",
    "\n",
    "Perlexity = 1.06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Time Executions\n",
    "\n",
    "1. Câu hỏi 1: Tổng quan về ngành Điện tử - Viễn thông của Đại học Bách Khoa Hà Nội\n",
    "2. Câu hỏi 2: Thời tiết ở Hà Nội trong 3 ngày tới như thế nào?\n",
    "3. Câu hỏi 3: Sự phát triển của chíp bán dẫn ở Việt Nam như thế nào?\n",
    "\n",
    "#### Đối với nhiệm vụ viết báo cáo nghiên cứu\n",
    "\n",
    "|Time (s) | T1 | T2 | T3|\n",
    "|----|----|----|----|\n",
    "|    | 140.212| 106.878 | 140.509|\n",
    "\n",
    "#### Đối với nhiệm vụ phân tích nguồn tham khảo\n",
    "\n",
    "|Time (s) | T1 | T2 | T3|\n",
    "|----|----|----| ----|\n",
    "|    |151.931  | 145.522 | 185.061|\n",
    "\n",
    "#### Đôi với nhiệm vụ viết khung báo cáo \n",
    "\n",
    "|Time (s) | T1 | T2 | T3|\n",
    "|----|----|----| ----|\n",
    "|    | 116.042 | 97.370 | 109.562|\n",
    "\n",
    "#### Đối với nhiệm vụ viết câu trả lời cho câu hỏi\n",
    "\n",
    "|Time (s) | T1 | T2 | T3|\n",
    "|----|----|----| ----|\n",
    "|    | 101.072 | 56.558 | 103.992|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "1. Summarization is a complex task, even advanced models can struggle to accurately capture all the important information from a source text.\n",
    "\n",
    "2. BERTScore or ROUGE that these metrics focus on things like including keywords from the source text, so that do not perfectly reflect human judgement of a good summary.\n",
    "\n",
    "3. The response of chatbot from many different sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HUMAN EVALUATION IS THE BEST WAY TO EVALUATE THE QUALITY OF A SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
